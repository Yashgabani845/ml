{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashgabani845/ml/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic NLP Concepts**"
      ],
      "metadata": {
        "id": "_lNY2NN0qdru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing:\n"
      ],
      "metadata": {
        "id": "Ta9ddEV9qfc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization in NLP**\n",
        "Tokenization is the process of breaking down text into smaller components\n",
        "like words or sentences. These components are called \"tokens\". Tokenization is one of the fundamental steps in natural language processing (NLP).\n",
        "\n",
        "\n",
        "\n",
        "Type of tokenization\n",
        "1.   Word\n",
        "2.   Sentence\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3G1Ykcbaqv6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp6qEzB1rGW-",
        "outputId": "d2531452-a889-4e01-8042-6f0b220ec0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "sgDO7fEkrP3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # Download the tokenizer models\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "teststring = \"long-off long-off suryakumar yadav suryakumar yadav.... ne pakada hai apne careear ka sabse important catch \"\n",
        "\n",
        "tokens = word_tokenize(teststring)\n",
        "tokens"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRzyOPWarT_o",
        "outputId": "778dfb69-4553-4115-b01d-575dfaf22b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['long-off',\n",
              " 'long-off',\n",
              " 'suryakumar',\n",
              " 'yadav',\n",
              " 'suryakumar',\n",
              " 'yadav',\n",
              " '....',\n",
              " 'ne',\n",
              " 'pakada',\n",
              " 'hai',\n",
              " 'apne',\n",
              " 'careear',\n",
              " 'ka',\n",
              " 'sabse',\n",
              " 'important',\n",
              " 'catch']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentance Tokenization**"
      ],
      "metadata": {
        "id": "O5kVxHJLr5ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentances = sent_tokenize(teststring)\n",
        "sentances"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rniDG-u2r95q",
        "outputId": "170e2fac-649d-40c6-e14c-b73eb3559b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['long-off long-off suryakumar yadav suryakumar yadav.... ne pakada hai apne careear ka sabse important catch']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***With spacy***\n",
        "\n"
      ],
      "metadata": {
        "id": "fgmfHuH6sNDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "rkiWR_iVsTkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(teststring)\n",
        "# Word Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(sentences)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJtBlRO9sXHx",
        "outputId": "5d48cc83-b81b-4270-8aa2-d4a5149d5378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['long', '-', 'off', 'long', '-', 'off', 'suryakumar', 'yadav', 'suryakumar', 'yadav', '....', 'ne', 'pakada', 'hai', 'apne', 'careear', 'ka', 'sabse', 'important', 'catch']\n",
            "['long-off long-off suryakumar yadav suryakumar yadav....', 'ne pakada hai apne careear ka sabse important catch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> For particular tokenization i would say nltk is quite better\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Dc_d73Tsz7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Between NLTK and spaCy for Tokenization**\n",
        "\n",
        "\n",
        "Speed\n",
        "NLTK :Slower\t,Slightly more code required\t ,Rule-based and uses punkt model\t, Allows manual adjustments\n",
        "\n",
        "Spacy:\n",
        "Faster\n",
        "More intuitive API\n",
        "Based on rules and machine learning\n",
        "Highly customizable pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "zSqZTPcvtBEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming and Lemmatization in NLP**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LX6Xn-lAuI3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"ran\", \"easily\", \"fairly\", \"studies\", \"studying\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "tsOr_3SQuwmH",
        "outputId": "f93c0479-a50c-4308-9f6c-a0f57047b75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'ran', 'easili', 'fairli', 'studi', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "stemmed_words = [lancaster_stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "A8oyJYEJu4_O",
        "outputId": "ef59171b-bcd9-4a36-d66c-2a371ae703a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'ran', 'easy', 'fair', 'study', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The PorterStemmer is one of the most popular stemming algorithms, and it removes common suffixes such as \"ing\", \"ly\", etc.**\n",
        "\n",
        "**LancasterStemmer: Another stemming algorithm that is more aggressive.**"
      ],
      "metadata": {
        "id": "b-e9vulGvrM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization in NLTK**"
      ],
      "metadata": {
        "id": "ffAs-2oev40N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization gives valid words by considering the word’s context and part of speech."
      ],
      "metadata": {
        "id": "wqnpdDHMwSnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specifying the POS, lemmatize() assumes the word is a noun. Providing the correct POS (e.g., pos='v' for verbs) leads to better results."
      ],
      "metadata": {
        "id": "CPByhcl5wcdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # For multilingual WordNet\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"ran\", \"easily\", \"fairly\", \"studies\", \"studying\"]\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "8MWCnD-_wUFh",
        "outputId": "e53c75eb-c443-4b25-9151-646158ab5c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'easily', 'fairly', 'study', 'study']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords Removal: This involves removing common words that do not add significant meaning to the text. These are usually articles, prepositions, and auxiliary verbs like “the”, “is”, “in”, etc.**"
      ],
      "metadata": {
        "id": "cE6XUpjuwdiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a simple sentence with some common words like 'the' and 'is'.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "aA4T3YpfxEIl",
        "outputId": "ae4ac8f8-2c59-4c62-b02c-5fa9b8bc3f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'sentence', 'common', 'words', 'like', \"'the\", \"'\", \"'is\", \"'\", '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TNQdkV1Vxc08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalization in NLTK**"
      ],
      "metadata": {
        "id": "4IGYctc7xVu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example sentence, with PUNCTUATION! And special characters like @ and #.\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text_lower = text.lower()\n",
        "\n",
        "# Remove punctuation and special characters using regex\n",
        "text_clean = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "tokens = word_tokenize(text_clean)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "lw6iAFESxNnU",
        "outputId": "d85ac9ef-3b2a-457e-d410-06928f4d15e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'an', 'example', 'sentence', 'with', 'punctuation', 'and', 'special', 'characters', 'like', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Extraction**"
      ],
      "metadata": {
        "id": "J5xtc_YsxkKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding how to represent text in a form that machine learning models can understand.\n",
        "\n"
      ],
      "metadata": {
        "id": "tykb_ieJxoQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **The Bag of Words (BoW)** model is a simple and widely used method for text representation. In this approach, text (such as a sentence or document) is represented as a collection of words, without considering the grammar, order, or structure. Each unique word in the corpus is used as a feature, and the value for each word is typically its frequency (how often it appears) in the document."
      ],
      "metadata": {
        "id": "ZP66pZKNxrqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Never jump over the lazy dog quickly\",\n",
        "]\n",
        "\n",
        "# Tokenize each document\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Create a vocabulary (unique words)\n",
        "vocabulary = set([word for doc in tokenized_docs for word in doc])\n",
        "\n",
        "# Create a frequency vector for each document\n",
        "def create_bow(doc, vocabulary):\n",
        "    bow_vector = Counter(doc)\n",
        "    return [bow_vector.get(word, 0) for word in vocabulary]\n",
        "\n",
        "# Generate the BoW vectors for all documents\n",
        "bow_vectors = [create_bow(doc, vocabulary) for doc in tokenized_docs]\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"BoW Vectors:\", bow_vectors)"
      ],
      "metadata": {
        "id": "74uWK4reyGy0",
        "outputId": "d347a0a5-1f69-4780-b965-7107823def0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'jump', 'quick', 'never', 'over', 'the', 'dog', 'fox', 'quickly', 'brown', 'lazy', 'jumps'}\n",
            "BoW Vectors: [[0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1], [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skitlearn**"
      ],
      "metadata": {
        "id": "YSfCBHuLzJKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Never jump over the lazy dog quickly\",\n",
        "]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the model to the documents and transform into BoW vectors\n",
        "bow_vectors = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the sparse matrix to a dense matrix for better readability\n",
        "bow_dense = bow_vectors.toarray()\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"BoW Vectors:\", bow_dense)\n"
      ],
      "metadata": {
        "id": "mJjGCBsqzPEL",
        "outputId": "a3bb8076-ff10-4449-c6fe-9eece3dfcc0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['brown' 'dog' 'fox' 'jump' 'jumps' 'lazy' 'never' 'over' 'quick'\n",
            " 'quickly' 'the']\n",
            "BoW Vectors: [[1 1 1 0 1 1 0 1 1 0 2]\n",
            " [0 1 0 1 0 1 1 1 0 1 1]]\n"
          ]
        }
      ]
    }
  ]
}