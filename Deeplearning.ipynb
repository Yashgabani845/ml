{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwcy8yppOHsfQcQ9mnruNq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yashgabani845/ml/blob/main/Deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qENsbqIQlEg5",
        "outputId": "8e3b1767-1e9a-4050-8736-794345ba0fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8743 - loss: 0.4281 - val_accuracy: 0.9599 - val_loss: 0.1313\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.1091 - val_accuracy: 0.9704 - val_loss: 0.0885\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0697 - val_accuracy: 0.9729 - val_loss: 0.0911\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9834 - loss: 0.0524 - val_accuracy: 0.9770 - val_loss: 0.0762\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9870 - loss: 0.0389 - val_accuracy: 0.9739 - val_loss: 0.0851\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0320 - val_accuracy: 0.9786 - val_loss: 0.0737\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9909 - loss: 0.0270 - val_accuracy: 0.9777 - val_loss: 0.0786\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0218 - val_accuracy: 0.9776 - val_loss: 0.0756\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9938 - loss: 0.0184 - val_accuracy: 0.9744 - val_loss: 0.0876\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9944 - loss: 0.0162 - val_accuracy: 0.9800 - val_loss: 0.0860\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9770 - loss: 0.1008\n",
            "Test accuracy: 0.9800000190734863\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "#we are normalising the value of the pixcels between 0 to 1 from 0 to 255\n",
        "# now we have to define the model architecture\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28)), #flatten is converting the array of 28x28 into 784 1D array\n",
        "    Dense(128, activation='relu'),  # First hidden layer with 64 neurons\n",
        "    Dense(64, activation='relu'),    # Second hidden layer with 64 neurons\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 neurons (one for each digit)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vy6V4_LjeJHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_test[9], cmap='gray')\n",
        "plt.title(\"Image at index 0\")\n",
        "plt.show()\n",
        "prediction = model.predict(x_test[0].reshape(1, 28, 28))\n",
        "predicted_digit = prediction.argmax()\n",
        "print(f\"Model predicts: {predicted_digit}\")\n",
        "print(f\"Actual digit: {y_test[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "zwdDgCtRtZlr",
        "outputId": "24c5ea80-5c3c-4d77-fa22-5be32816f7c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Model predicts: 7\n",
            "Actual digit: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sample data (replace with actual churn dataset)\n",
        "# Let's assume 'X' contains the customer data and 'y' is the target (churn: 0 or 1)\n",
        "X = np.array([[35, 1, 500, 2], [40, 0, 1000, 3], [25, 1, 1500, 1]])  # Example features\n",
        "y = np.array([0, 1, 0])  # Churn (0 = No, 1 = Yes)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create the ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "\n",
        "# Add second hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Output layer (binary classification: churn or not)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
        "\n",
        "# Evaluate additional metrics (optional)\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wIhkx6eKgW",
        "outputId": "03f5cb47-e075-472d-ccde-9131e147feff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 0.5386 - val_accuracy: 0.0000e+00 - val_loss: 0.7496\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697ms/step - accuracy: 1.0000 - loss: 0.5159 - val_accuracy: 0.0000e+00 - val_loss: 0.7692\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.4942 - val_accuracy: 0.0000e+00 - val_loss: 0.7884\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.4732 - val_accuracy: 0.0000e+00 - val_loss: 0.8076\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.4527 - val_accuracy: 0.0000e+00 - val_loss: 0.8269\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.4328 - val_accuracy: 0.0000e+00 - val_loss: 0.8481\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.4142 - val_accuracy: 0.0000e+00 - val_loss: 0.8694\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.3963 - val_accuracy: 0.0000e+00 - val_loss: 0.8907\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.3790 - val_accuracy: 0.0000e+00 - val_loss: 0.9121\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.3629 - val_accuracy: 0.0000e+00 - val_loss: 0.9331\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.3473 - val_accuracy: 0.0000e+00 - val_loss: 0.9543\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 0.3322 - val_accuracy: 0.0000e+00 - val_loss: 0.9753\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 1.0000 - loss: 0.3177 - val_accuracy: 0.0000e+00 - val_loss: 0.9962\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.3037 - val_accuracy: 0.0000e+00 - val_loss: 1.0171\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2903 - val_accuracy: 0.0000e+00 - val_loss: 1.0376\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.2774 - val_accuracy: 0.0000e+00 - val_loss: 1.0566\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.2650 - val_accuracy: 0.0000e+00 - val_loss: 1.0754\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.2531 - val_accuracy: 0.0000e+00 - val_loss: 1.0942\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2415 - val_accuracy: 0.0000e+00 - val_loss: 1.1129\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.2303 - val_accuracy: 0.0000e+00 - val_loss: 1.1316\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.2196 - val_accuracy: 0.0000e+00 - val_loss: 1.1498\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.2093 - val_accuracy: 0.0000e+00 - val_loss: 1.1679\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.1995 - val_accuracy: 0.0000e+00 - val_loss: 1.1862\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.1906 - val_accuracy: 0.0000e+00 - val_loss: 1.2049\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.1822 - val_accuracy: 0.0000e+00 - val_loss: 1.2237\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1740 - val_accuracy: 0.0000e+00 - val_loss: 1.2426\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1661 - val_accuracy: 0.0000e+00 - val_loss: 1.2615\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1583 - val_accuracy: 0.0000e+00 - val_loss: 1.2805\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.1508 - val_accuracy: 0.0000e+00 - val_loss: 1.2995\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1436 - val_accuracy: 0.0000e+00 - val_loss: 1.3185\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1368 - val_accuracy: 0.0000e+00 - val_loss: 1.3371\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.1303 - val_accuracy: 0.0000e+00 - val_loss: 1.3556\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1241 - val_accuracy: 0.0000e+00 - val_loss: 1.3740\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.1182 - val_accuracy: 0.0000e+00 - val_loss: 1.3922\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1126 - val_accuracy: 0.0000e+00 - val_loss: 1.4098\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.1072 - val_accuracy: 0.0000e+00 - val_loss: 1.4274\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.1020 - val_accuracy: 0.0000e+00 - val_loss: 1.4448\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.0971 - val_accuracy: 0.0000e+00 - val_loss: 1.4621\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 0.0925 - val_accuracy: 0.0000e+00 - val_loss: 1.4790\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0881 - val_accuracy: 0.0000e+00 - val_loss: 1.4958\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0839 - val_accuracy: 0.0000e+00 - val_loss: 1.5124\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0799 - val_accuracy: 0.0000e+00 - val_loss: 1.5289\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0761 - val_accuracy: 0.0000e+00 - val_loss: 1.5452\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0725 - val_accuracy: 0.0000e+00 - val_loss: 1.5613\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0691 - val_accuracy: 0.0000e+00 - val_loss: 1.5772\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0659 - val_accuracy: 0.0000e+00 - val_loss: 1.5939\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0629 - val_accuracy: 0.0000e+00 - val_loss: 1.6116\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0600 - val_accuracy: 0.0000e+00 - val_loss: 1.6291\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.0573 - val_accuracy: 0.0000e+00 - val_loss: 1.6464\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0547 - val_accuracy: 0.0000e+00 - val_loss: 1.6635\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 1.6635\n",
            "Test Accuracy: 0.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       1.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative (for backpropagation)\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Binary cross-entropy loss function\n",
        "def binary_crossentropy(y_true, y_pred):\n",
        "    # Adding a small epsilon to prevent log(0)\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Custom Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases with small random values\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Weights between input and hidden layer\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "\n",
        "        # Weights between hidden and output layer\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Forward pass through the network\n",
        "        self.hidden_input = np.dot(X, self.W1) + self.b1\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "\n",
        "        self.final_input = np.dot(self.hidden_output, self.W2) + self.b2\n",
        "        self.final_output = sigmoid(self.final_input)\n",
        "\n",
        "        return self.final_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Backward pass (gradient computation)\n",
        "        # Calculate the error\n",
        "        output_error = self.final_output - y\n",
        "        output_delta = output_error * sigmoid_derivative(self.final_output)\n",
        "\n",
        "        hidden_error = output_delta.dot(self.W2.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        self.W2 -= self.hidden_output.T.dot(output_delta) * learning_rate\n",
        "        self.b2 -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        self.W1 -= X.T.dot(hidden_delta) * learning_rate\n",
        "        self.b1 -= np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        # Train the network using backpropagation\n",
        "        for epoch in range(epochs):\n",
        "            # Perform forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Perform backward pass\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Optionally print the loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(binary_crossentropy(y, output))\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict the output for the input data\n",
        "        return self.forward(X)\n",
        "\n",
        "# Sample Data: X is input features, y is the target (0 or 1)\n",
        "# Let's assume a simple binary classification problem where we have two features\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0], [1], [1], [0]])  # XOR problem\n",
        "\n",
        "# Initialize and train the neural network\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained model\n",
        "predictions = nn.predict(X)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n",
        "\n",
        "# Convert predictions to binary (0 or 1)\n",
        "predictions_binary = (predictions > 0.5).astype(int)\n",
        "print(\"Binary Predictions:\")\n",
        "print(predictions_binary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KAHAjbkitt5",
        "outputId": "14c19e57-5cc2-4081-e0a2-da49a5709e80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.8738388482853707\n",
            "Epoch 100, Loss: 0.6739434460831092\n",
            "Epoch 200, Loss: 0.6609642276158595\n",
            "Epoch 300, Loss: 0.6472473118500764\n",
            "Epoch 400, Loss: 0.6307429590156646\n",
            "Epoch 500, Loss: 0.6106457370689734\n",
            "Epoch 600, Loss: 0.5865865174580249\n",
            "Epoch 700, Loss: 0.5583304609414986\n",
            "Epoch 800, Loss: 0.5257060584167189\n",
            "Epoch 900, Loss: 0.4888235218357234\n",
            "Epoch 1000, Loss: 0.44851710017561014\n",
            "Epoch 1100, Loss: 0.406599692132177\n",
            "Epoch 1200, Loss: 0.3654889301245083\n",
            "Epoch 1300, Loss: 0.3273667892701142\n",
            "Epoch 1400, Loss: 0.2935452670098068\n",
            "Epoch 1500, Loss: 0.26440093076961035\n",
            "Epoch 1600, Loss: 0.23967564382656642\n",
            "Epoch 1700, Loss: 0.21881611307264803\n",
            "Epoch 1800, Loss: 0.20120144551653435\n",
            "Epoch 1900, Loss: 0.1862557772015527\n",
            "Epoch 2000, Loss: 0.17348833257674984\n",
            "Epoch 2100, Loss: 0.1624976483376045\n",
            "Epoch 2200, Loss: 0.15296155891285312\n",
            "Epoch 2300, Loss: 0.14462341512673538\n",
            "Epoch 2400, Loss: 0.13727888069511296\n",
            "Epoch 2500, Loss: 0.13076473939097602\n",
            "Epoch 2600, Loss: 0.12494991238396655\n",
            "Epoch 2700, Loss: 0.11972843603857236\n",
            "Epoch 2800, Loss: 0.11501403722232781\n",
            "Epoch 2900, Loss: 0.1107359593731056\n",
            "Epoch 3000, Loss: 0.1068357490197751\n",
            "Epoch 3100, Loss: 0.10326477352935558\n",
            "Epoch 3200, Loss: 0.09998229421560897\n",
            "Epoch 3300, Loss: 0.09695396176453813\n",
            "Epoch 3400, Loss: 0.0941506339194454\n",
            "Epoch 3500, Loss: 0.09154744026109352\n",
            "Epoch 3600, Loss: 0.08912303751697578\n",
            "Epoch 3700, Loss: 0.08685901267638713\n",
            "Epoch 3800, Loss: 0.08473940149008831\n",
            "Epoch 3900, Loss: 0.08275029761770814\n",
            "Epoch 4000, Loss: 0.08087953343918697\n",
            "Epoch 4100, Loss: 0.0791164178740019\n",
            "Epoch 4200, Loss: 0.0774515198240471\n",
            "Epoch 4300, Loss: 0.0758764883440561\n",
            "Epoch 4400, Loss: 0.0743839025461725\n",
            "Epoch 4500, Loss: 0.07296714570887756\n",
            "Epoch 4600, Loss: 0.07162029919285641\n",
            "Epoch 4700, Loss: 0.07033805264753282\n",
            "Epoch 4800, Loss: 0.0691156276815308\n",
            "Epoch 4900, Loss: 0.06794871271287473\n",
            "Epoch 5000, Loss: 0.0668334071439468\n",
            "Epoch 5100, Loss: 0.06576617334751993\n",
            "Epoch 5200, Loss: 0.0647437952229668\n",
            "Epoch 5300, Loss: 0.06376334230082098\n",
            "Epoch 5400, Loss: 0.06282213855064711\n",
            "Epoch 5500, Loss: 0.06191773519048241\n",
            "Epoch 5600, Loss: 0.06104788691278931\n",
            "Epoch 5700, Loss: 0.06021053103726464\n",
            "Epoch 5800, Loss: 0.05940376917918322\n",
            "Epoch 5900, Loss: 0.058625851086523004\n",
            "Epoch 6000, Loss: 0.05787516035254858\n",
            "Epoch 6100, Loss: 0.057150201754906954\n",
            "Epoch 6200, Loss: 0.05644959000927381\n",
            "Epoch 6300, Loss: 0.05577203975652799\n",
            "Epoch 6400, Loss: 0.055116356628388055\n",
            "Epoch 6500, Loss: 0.05448142925830936\n",
            "Epoch 6600, Loss: 0.05386622212288711\n",
            "Epoch 6700, Loss: 0.0532697691146575\n",
            "Epoch 6800, Loss: 0.05269116776046522\n",
            "Epoch 6900, Loss: 0.052129574010884326\n",
            "Epoch 7000, Loss: 0.05158419753584749\n",
            "Epoch 7100, Loss: 0.05105429746991794\n",
            "Epoch 7200, Loss: 0.05053917855775268\n",
            "Epoch 7300, Loss: 0.050038187656421367\n",
            "Epoch 7400, Loss: 0.04955071055653568\n",
            "Epoch 7500, Loss: 0.049076169088707336\n",
            "Epoch 7600, Loss: 0.048614018485818575\n",
            "Epoch 7700, Loss: 0.048163744975026845\n",
            "Epoch 7800, Loss: 0.04772486357642953\n",
            "Epoch 7900, Loss: 0.04729691608792112\n",
            "Epoch 8000, Loss: 0.04687946923806832\n",
            "Epoch 8100, Loss: 0.04647211299082828\n",
            "Epoch 8200, Loss: 0.04607445898769366\n",
            "Epoch 8300, Loss: 0.04568613911439452\n",
            "Epoch 8400, Loss: 0.045306804180647094\n",
            "Epoch 8500, Loss: 0.044936122702645326\n",
            "Epoch 8600, Loss: 0.044573779779046876\n",
            "Epoch 8700, Loss: 0.04421947605215146\n",
            "Epoch 8800, Loss: 0.04387292674680114\n",
            "Epoch 8900, Loss: 0.0435338607802725\n",
            "Epoch 9000, Loss: 0.04320201993709193\n",
            "Epoch 9100, Loss: 0.0428771581032914\n",
            "Epoch 9200, Loss: 0.04255904055514447\n",
            "Epoch 9300, Loss: 0.042247443297895204\n",
            "Epoch 9400, Loss: 0.04194215245040725\n",
            "Epoch 9500, Loss: 0.04164296367203721\n",
            "Epoch 9600, Loss: 0.04134968162837547\n",
            "Epoch 9700, Loss: 0.04106211949279753\n",
            "Epoch 9800, Loss: 0.040780098481042554\n",
            "Epoch 9900, Loss: 0.0405034474162804\n",
            "Predictions:\n",
            "[[0.02985032]\n",
            " [0.95647456]\n",
            " [0.96242871]\n",
            " [0.04670104]]\n",
            "Binary Predictions:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    }
  ]
}